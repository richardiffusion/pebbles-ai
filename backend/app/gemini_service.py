# backend/app/gemini_service.py (æˆ– ai_service.py)

import json
import uuid
import time
import google.generativeai as genai
from openai import AsyncOpenAI
from app.database import settings
from app.models import Pebble, LevelContent, MainBlock, SidebarBlock

# ==========================================
# 1. é€šç”¨è¾…åŠ©å‡½æ•° (æ•°æ®æ¸…æ´—) - ä¿æŒä¸å˜
# ==========================================
def _process_json_to_pebble(data: dict, topic: str) -> Pebble:
    """å°† AI è¿”å›žçš„åŽŸå§‹ JSON è½¬æ¢ä¸ºæ ‡å‡†çš„ Pebble å¯¹è±¡"""
    
    def sanitize_sidebar_block(sb: dict) -> SidebarBlock:
        if 'heading' not in sb: sb['heading'] = sb.get('title', "Info")
        valid_types = ['definition', 'profile', 'stat']
        if 'type' not in sb or sb['type'] not in valid_types: sb['type'] = 'definition'
        if 'body' not in sb: sb['body'] = sb.get('description', '') or "No content"
        return SidebarBlock(**sb)

    def process_content(content_data):
        main_blocks = []
        for b in content_data.get('mainContent', []):
            if b.get('type') == 'key_points' and isinstance(b.get('body'), str):
                b['body'] = [s.strip() for s in b['body'].split('|')]
            if 'iconType' not in b: b['iconType'] = 'default'
            main_blocks.append(MainBlock(**b))
        
        sidebar_blocks = []
        for sb in content_data.get('sidebarContent', []):
            try:
                sidebar_blocks.append(sanitize_sidebar_block(sb))
            except Exception:
                pass

        # ç§»é™¤å·²å¤„ç†å­—æ®µï¼Œé˜²æ­¢ä¼ å‚å†²çª
        clean_data = {k: v for k, v in content_data.items() if k not in ['mainContent', 'sidebarContent']}
        
        return LevelContent(
            **clean_data,
            mainContent=main_blocks,
            sidebarContent=sidebar_blocks
        )

    return Pebble(
        id=str(uuid.uuid4()),
        topic=topic,
        timestamp=time.time() * 1000,
        content={
            "ELI5": process_content(data.get('eli5_content', {})),
            "ACADEMIC": process_content(data.get('academic_content', {}))
        },
        socraticQuestions=data.get('socratic_questions', [])
    )

def _build_prompt(topic: str, context_pebbles: list) -> str:
    context_str = ""
    if context_pebbles:
        context_str = "CONTEXT NODES:\n" + "\n".join(
            [f"- {p['topic']}: {p['content']['ELI5']['summary']}" for p in context_pebbles]
        )
        
    return f"""
    You are 'Pebbles', a Cognitive Architect.
    Topic: "{topic}"
    {context_str}

    Generate a high-density, magazine-style knowledge artifact.
    OUTPUT MUST BE RAW JSON. NO MARKDOWN.
    
    REQUIRED JSON STRUCTURE:
    {{
      "eli5_content": {{
        "title": "...", "summary": "...", "emojiCollage": ["e1", "e2"],
        "mainContent": [ {{ "type": "text", "heading": "...", "body": "...", "iconType": "idea" }} ],
        "sidebarContent": [ {{ "type": "definition", "heading": "...", "body": "..." }} ],
        "keywords": ["..."]
      }},
      "academic_content": {{ ... same structure ... }},
      "socratic_questions": ["Q1?", "Q2?"]
    }}
    """

# ==========================================
# 2. DeepSeek å®žçŽ°
# ==========================================
async def _generate_with_deepseek(topic: str, context_pebbles: list) -> Pebble:
    client = AsyncOpenAI(api_key=settings.DEEPSEEK_API_KEY, base_url=settings.DEEPSEEK_BASE_URL)
    prompt = _build_prompt(topic, context_pebbles)
    
    response = await client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "You output VALID JSON only."},
            {"role": "user", "content": prompt},
        ],
        response_format={ "type": "json_object" },
        temperature=1.3,
    )
    
    content_str = response.choices[0].message.content
    if content_str.startswith("```json"):
        content_str = content_str.replace("```json", "").replace("```", "")
        
    return _process_json_to_pebble(json.loads(content_str), topic)

async def _rewrite_with_deepseek(text: str, instruction: str) -> str:
    client = AsyncOpenAI(api_key=settings.DEEPSEEK_API_KEY, base_url=settings.DEEPSEEK_BASE_URL)
    response = await client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "You are an expert editor. Output ONLY the rewritten text."},
            {"role": "user", "content": f"Instruction: {instruction}\n\nOriginal: {text}"},
        ],
        temperature=0.7,
    )
    return response.choices[0].message.content.strip()

# ==========================================
# 3. Gemini å®žçŽ° (å¢žå¼ºä¿®å¤ç‰ˆ)
# ==========================================
async def _generate_with_gemini(topic: str, context_pebbles: list) -> Pebble:
    genai.configure(api_key=settings.GEMINI_API_KEY)
    
    # å°è¯•ä¸åŒçš„æ¨¡åž‹åç§°ï¼Œé˜²æ­¢ç‰ˆæœ¬å·®å¼‚
    model_candidates = ['gemini-2.5-flash', 'gemini-1.5-flash-latest', 'gemini-1.5-pro']
    model = None
    
    for model_name in model_candidates:
        try:
            model = genai.GenerativeModel(
                model_name,
                generation_config={"response_mime_type": "application/json"}
            )
            break
        except Exception:
            continue
            
    if not model:
        # å¦‚æžœè¿˜æ²¡æ‰¾åˆ°ï¼Œå›žé€€åˆ°é»˜è®¤å¹¶åŽ»æŽ‰ json é…ç½®ï¼ˆé˜²æ­¢æ—§åº“æŠ¥é”™ï¼‰
        print("Warning: Could not configure JSON mode for Gemini 1.5. Falling back to default.")
        model = genai.GenerativeModel('gemini-pro')

    prompt = _build_prompt(topic, context_pebbles)
    
    try:
        response = await model.generate_content_async(prompt)
        text = response.text
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ markdown (å¦‚æžœå›žé€€åˆ°äº†éž JSON æ¨¡å¼)
        if text.startswith("```json"):
            text = text.replace("```json", "").replace("```", "")
        return _process_json_to_pebble(json.loads(text), topic)
        
    except Exception as e:
        # â˜…â˜…â˜… è°ƒè¯•ï¼šåˆ—å‡ºå¯ç”¨æ¨¡åž‹ï¼Œæ–¹ä¾¿æŽ’æŸ¥ 404 é”™è¯¯ â˜…â˜…â˜…
        print(f"\n--- GEMINI ERROR DEBUG ---")
        print(f"Error: {e}")
        try:
            print("Available models for your key:")
            for m in genai.list_models():
                if 'generateContent' in m.supported_generation_methods:
                    print(f"- {m.name}")
        except:
            print("Could not list models.")
        print(f"--------------------------\n")
        raise e

async def _rewrite_with_gemini(text: str, instruction: str) -> str:
    genai.configure(api_key=settings.GEMINI_API_KEY)
    
    # â˜…â˜…â˜… ä¿®å¤ï¼šå¢žåŠ æ¨¡åž‹å›žé€€æœºåˆ¶ (åŒ generate å‡½æ•°) â˜…â˜…â˜…
    model_candidates = ['gemini-2.5-flash', 'gemini-1.5-flash-latest', 'gemini-1.5-pro', 'gemini-pro']
    model = None
    
    for model_name in model_candidates:
        try:
            # å°è¯•åˆå§‹åŒ–æ¨¡åž‹
            m = genai.GenerativeModel(model_name)
            # è¿™æ˜¯ä¸€ä¸ªè½»é‡çº§æ£€æŸ¥ï¼Œå¦‚æžœæ¨¡åž‹åä¸å¯¹ï¼Œæœ‰äº›åº“ç‰ˆæœ¬ä¼šåœ¨è°ƒç”¨æ—¶æ‰æŠ¥é”™
            # ä½†æˆ‘ä»¬åœ¨ä¸‹é¢ try-catch è°ƒç”¨ generate_content_async
            model = m
            break
        except Exception:
            continue
    
    # å¦‚æžœæ²¡æ‰¾åˆ°ï¼Œå…œåº•ä½¿ç”¨ gemini-pro
    if not model:
        model = genai.GenerativeModel('gemini-pro')

    prompt = f"""
    You are an expert editor. 
    Instruction: {instruction}
    
    Original Text:
    {text}
    
    Output ONLY the rewritten text. No preamble.
    """
    
    try:
        response = await model.generate_content_async(prompt)
        return response.text.strip()
    except Exception as e:
        print(f"Gemini Rewrite Error: {e}")
        # å¦‚æžœè¿˜æ˜¯å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸ï¼Œä½†åœ¨æ—¥å¿—é‡Œèƒ½çœ‹åˆ°å…·ä½“åŽŸå› 
        raise e

# ==========================================
# 4. ä¸»å…¥å£
# ==========================================
async def generate_pebble_logic(topic: str, context_pebbles: list) -> Pebble:
    provider = settings.AI_PROVIDER.lower()
    print(f"ðŸŒŠ Generating using Provider: {provider.upper()}")
    
    try:
        if provider == 'deepseek':
            return await _generate_with_deepseek(topic, context_pebbles)
        else:
            return await _generate_with_gemini(topic, context_pebbles)
    except Exception as e:
        print(f"AI Generation Error ({provider}): {e}")
        raise e

async def rewrite_text_logic(text: str, mode: str) -> str:
    instructions = {
        "improve": "Rewrite to be more clear, professional, and engaging.",
        "shorter": "Summarize concisely. Remove fluff.",
        "longer": "Expand with more detail and context.",
        "simplify": "Explain like I'm 5 years old."
    }
    instruction = instructions.get(mode, instructions["improve"])
    
    provider = settings.AI_PROVIDER.lower()
    
    try:
        if provider == 'deepseek':
            return await _rewrite_with_deepseek(text, instruction)
        else:
            return await _rewrite_with_gemini(text, instruction)
    except Exception as e:
        print(f"Rewrite Error ({provider}): {e}")
        raise e